{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.ssd_mobilenet import ssd_300\n",
    "from misc.keras_ssd_loss import SSDLoss, FocalLoss, weightedSSDLoss, weightedFocalLoss\n",
    "from misc.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from misc.keras_layer_L2Normalization import L2Normalization\n",
    "from misc.ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "from misc.ssd_batch_generator import BatchGenerator\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300  # Height of the input images\n",
    "img_width = 300  # Width of the input images\n",
    "img_channels = 3  # Number of color channels of the input images\n",
    "subtract_mean = [123, 117, 104]  # The per-channel mean of the images in the dataset\n",
    "swap_channels = True  # The color channel order in the original SSD is BGR\n",
    "n_classes = 20  # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_voc = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88,\n",
    "              1.05]  # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87,\n",
    "               1.05]  # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_voc\n",
    "\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0 / 3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0 / 3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0 / 3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]]  # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300]  # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "           0.5]  # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "limit_boxes = False  # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2,\n",
    "             0.2]  # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "coords = 'centroids'  # Whether the box coordinates to be used as targets for the model should be in the 'centroids', 'corners', or 'minmax' format, see documentation\n",
    "normalize_coords = True\n",
    "\n",
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session()  # Clear previous models from memory.\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "      if epoch <= 200:\n",
    "          return 0.001\n",
    "      else:\n",
    "          return 0.0001\n",
    "\n",
    "\n",
    "def train():\n",
    "  model = ssd_300(mode = 'training',\n",
    "                  image_size=(img_height, img_width, img_channels),\n",
    "                  n_classes=n_classes,\n",
    "                  l2_regularization=0.0005,\n",
    "                  scales=scales,\n",
    "                  aspect_ratios_per_layer=aspect_ratios,\n",
    "                  two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                  steps=steps,\n",
    "                  offsets=offsets,\n",
    "                  limit_boxes=limit_boxes,\n",
    "                  variances=variances,\n",
    "                  coords=coords,\n",
    "                  normalize_coords=normalize_coords,\n",
    "                  subtract_mean=subtract_mean,\n",
    "                  divide_by_stddev=None,\n",
    "                  swap_channels=swap_channels)\n",
    "\n",
    "  \n",
    "\n",
    "  model.load_weights('mobilenet_1_0_224_tf.h5', by_name=True,skip_mismatch=True)\n",
    "\n",
    "\n",
    "  predictor_sizes = [model.get_layer('conv11_mbox_conf').output_shape[1:3],\n",
    "                     model.get_layer('conv13_mbox_conf').output_shape[1:3],\n",
    "                     model.get_layer('conv14_2_mbox_conf').output_shape[1:3],\n",
    "                     model.get_layer('conv15_2_mbox_conf').output_shape[1:3],\n",
    "                     model.get_layer('conv16_2_mbox_conf').output_shape[1:3],\n",
    "                     model.get_layer('conv17_2_mbox_conf').output_shape[1:3]]\n",
    " \n",
    "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)\n",
    "\n",
    "  ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "\n",
    "  train_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "  val_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "\n",
    "  # 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "  # TODO: Set the paths to the datasets here.\n",
    "\n",
    "\n",
    "  VOC_2007_images_dir = 'data/training/'\n",
    "  VOC_2007_images_dir1 = 'data/val/'\n",
    "  \n",
    "\n",
    "  # The directories that contain the annotations.\n",
    "  VOC_2007_annotations_dir = 'annotrain/'\n",
    "  VOC_2007_annotations_dir1 = 'annoval/'\n",
    "  \n",
    "\n",
    "  # The paths to the image sets.\n",
    " # VOC_2007_train_image_set_filename = args.voc_dir_path + '/VOC2007/ImageSets/Main/trainval.txt'\n",
    "\n",
    "\n",
    " # VOC_2007_val_image_set_filename = args.voc_dir_path + '/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "\n",
    "  # The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "\n",
    "  classes = ['drinking','eating']\n",
    "  VOC_2007_train_image_set_filename= 'data1.txt'\n",
    "  VOC_2007_train_val_set_filename= 'data2.txt'\n",
    "\n",
    "\n",
    "  train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                          \n",
    "                          annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                          image_set_filenames=[VOC_2007_train_image_set_filename],\n",
    "                          classes=classes,\n",
    "                          include_classes='all',\n",
    "                          exclude_truncated=False,\n",
    "                          exclude_difficult=False,\n",
    "                          ret=False)\n",
    "    \n",
    "  val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir1],\n",
    "                        image_set_filenames=[VOC_2007_train_val_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir1],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "  # 3: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "  ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
    "                                  img_width=img_width,\n",
    "                                  n_classes=n_classes,\n",
    "                                  predictor_sizes=predictor_sizes,\n",
    "                                  min_scale=None,\n",
    "                                  max_scale=None,\n",
    "                                  scales=scales,\n",
    "                                  aspect_ratios_global=None,\n",
    "                                  aspect_ratios_per_layer=aspect_ratios,\n",
    "                                  two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                  steps=steps,\n",
    "                                  offsets=offsets,\n",
    "                                  limit_boxes=limit_boxes,\n",
    "                                  variances=variances,\n",
    "                                  pos_iou_threshold=0.5,\n",
    "                                  neg_iou_threshold=0.2,\n",
    "                                  coords=coords,\n",
    "                                  normalize_coords=normalize_coords)\n",
    "\n",
    "  batch_size = 32\n",
    "\n",
    "  train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           train=True,\n",
    "                                           ssd_box_encoder=ssd_box_encoder,\n",
    "                                           convert_to_3_channels=True,\n",
    "                                           equalize=False,\n",
    "                                           brightness=(0.5, 2, 0.5),\n",
    "                                           flip=0.5,\n",
    "                                           translate=False,\n",
    "                                           scale=False,\n",
    "                                           max_crop_and_resize=(img_height, img_width, 1, 3),\n",
    "                                           # This one is important because the Pascal VOC images vary in size\n",
    "                                           random_pad_and_resize=(img_height, img_width, 1, 3, 0.5),\n",
    "                                           # This one is important because the Pascal VOC images vary in size\n",
    "                                           random_crop=False,\n",
    "                                           crop=False,\n",
    "                                           resize=False,\n",
    "                                           gray=False,\n",
    "                                           limit_boxes=True,\n",
    "                                           # While the anchor boxes are not being clipped, the ground truth boxes should be\n",
    "                                           include_thresh=0.4)\n",
    "\n",
    "  val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           train=True,\n",
    "                                           ssd_box_encoder=ssd_box_encoder,\n",
    "                                           convert_to_3_channels=True,\n",
    "                                           equalize=False,\n",
    "                                           brightness=(0.5, 2, 0.5),\n",
    "                                           flip=0.5,\n",
    "                                           translate=False,\n",
    "                                           scale=False,\n",
    "                                           max_crop_and_resize=(img_height, img_width, 1, 3),\n",
    "                                           # This one is important because the Pascal VOC images vary in size\n",
    "                                           random_pad_and_resize=(img_height, img_width, 1, 3, 0.5),\n",
    "                                           # This one is important because the Pascal VOC images vary in size\n",
    "                                           random_crop=False,\n",
    "                                           crop=False,\n",
    "                                           resize=False,\n",
    "                                           gray=False,\n",
    "                                           limit_boxes=True,\n",
    "                                           # While the anchor boxes are not being clipped, the ground truth boxes should be\n",
    "                                           include_thresh=0.4)\n",
    "  # Get the number of samples in the training and validations datasets to compute the epoch lengths below.\n",
    "  n_train_samples = train_dataset.get_n_samples()\n",
    "  n_val_samples = val_dataset.get_n_samples()\n",
    "  learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule)\n",
    "   \n",
    "  checkpoint_path = \"ssd300_epoch-{epoch:02d}.h5\"\n",
    "\n",
    "  checkpoint = ModelCheckpoint(checkpoint_path)\n",
    "  \n",
    "  log_path =   \"/logs\"\n",
    " \n",
    "  tensorborad = TensorBoard(log_dir=log_path,\n",
    "                            histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "\n",
    "\n",
    "  callbacks = [checkpoint,tensorborad,learning_rate_scheduler]\n",
    "\n",
    "  # TODO: Set the number of epochs to train for.\n",
    "  epochs = 3\n",
    "  intial_epoch = 1\n",
    "  history = model.fit_generator(generator=train_generator,\n",
    "                                steps_per_epoch=ceil(n_train_samples)/batch_size,\n",
    "                                verbose=1,\n",
    "                                initial_epoch=intial_epoch,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=val_generator,\n",
    "                                validation_steps=1,\n",
    "                                callbacks=callbacks\n",
    "                                )\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "   \n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhin\\Anaconda3\\envs\\ml1P13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "conv1 shape:  (?, 150, 150, 64)\n",
      "conv3 shape:  (?, 75, 75, 128)\n",
      "conv5 shape:  (?, 38, 38, 256)\n",
      "conv11 shape:  (?, 19, 19, 512)\n",
      "conv13 shape:  (?, 10, 10, 1024)\n",
      "conv14 shape (?, 5, 5, 512)\n",
      "conv15 shape (?, 3, 3, 256)\n",
      "conv16 shape (?, 2, 2, 256)\n",
      "conv17 shape (?, 1, 1, 128)\n",
      "in training mode\n"
     ]
    }
   ],
   "source": [
    "model = ssd_300(\"training\",\n",
    "                image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                limit_boxes=limit_boxes,\n",
    "                variances=variances,\n",
    "                coords=coords,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=subtract_mean,\n",
    "                divide_by_stddev=127.5,\n",
    "                swap_channels=swap_channels)\n",
    "model.load_weights(\"ssd300_epoch-222.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/val/000001.jpg\n",
      "time taken by ssd 3.75723934173584\n",
      "data/val/000019 (2).jpg\n",
      "time taken by ssd 0.012933492660522461\n",
      "data/val/000021 (3).jpg\n",
      "time taken by ssd 0.012965917587280273\n",
      "data/val/000021.jpg\n",
      "time taken by ssd 0.011924028396606445\n",
      "data/val/000058.jpg\n",
      "time taken by ssd 0.013922929763793945\n",
      "data/val/000084 (2).jpg\n",
      "time taken by ssd 0.012931108474731445\n",
      "data/val/000085 (2).jpg\n",
      "time taken by ssd 0.013947725296020508\n",
      "data/val/000108.jpg\n",
      "time taken by ssd 0.012935400009155273\n",
      "data/val/000208.jpg\n",
      "time taken by ssd 0.012917518615722656\n",
      "data/val/1.jpg\n",
      "time taken by ssd 0.012965679168701172\n"
     ]
    }
   ],
   "source": [
    "dir_path='data/val/'\n",
    "for file in os.listdir(dir_path):\n",
    "    filename = dir_path + file\n",
    "\n",
    "    print(filename) \n",
    "\n",
    "    img1 = cv2.imread(filename)\n",
    "   # img = img.astype('uint8')\n",
    "   \n",
    "    img = cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "   \n",
    "    # # img1 = ima[90:390,160:460]\n",
    "    # img1 = cv2.resize(ima,dsize=(img_height,img_width))\n",
    "    # im = img1\n",
    "    orig_images = []  # Store the im ages here.\n",
    "    input_images = []  # Store resized versions of the images here.\n",
    "    orig_images.append(img1)\n",
    "\n",
    "    # img1 = image.img_to_array(img1)\n",
    "    # input_images.append(img1)\n",
    "    # input_images = np.array(input_images)\n",
    "\n",
    "\n",
    "\n",
    "    ima = img\n",
    "    # img = img[:,a:a+320]\n",
    "    image1 = cv2.resize(img,(300,300))\n",
    "    image1 = np.array(image1,dtype=np.float32)\n",
    "\n",
    "    # image1[:,:,0] = 0.007843*(image1[:,:,0] - 127.5)\n",
    "    # image1[:,:,1] = 0.007843*(image1[:,:,1] - 127.5)\n",
    "    # image1[:,:,2] = 0.007843*(image1[:,:,2] - 127.5)\n",
    "    # image1 = image1[:,:,::-1]\n",
    "\n",
    "    image1 = image1[np.newaxis,:,:,:]\n",
    "    # input_images.append(image1)\n",
    "    input_images = np.array(image1)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    y_pred = model.predict(input_images)\n",
    "    # print y_pred.shape\n",
    "    # y_pred = y_pred.flatten()\n",
    "    # print (y_pred[:15])\n",
    "\n",
    "    # print 'y_pred shape', y_pred.shape\n",
    "\n",
    "    print(\"time taken by ssd\", time.time() - start_time)\n",
    "\n",
    "    # confidence_threshold = 0.25\n",
    "\n",
    "    # y_pred_decoded = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "\n",
    "\n",
    "    y_pred_decoded = decode_y(y_pred,\n",
    "                              confidence_thresh=0.20,\n",
    "                              iou_threshold=0.45,\n",
    "                              top_k=100,\n",
    "                              input_coords='centroids',\n",
    "                              normalize_coords=True,\n",
    "                              img_height=img_height,\n",
    "                              img_width=img_width)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    for box in y_pred_decoded[0]:\n",
    "\n",
    "        xmin = int(box[-4] * orig_images[0].shape[1] / img_width)\n",
    "        ymin = int(box[-3] * orig_images[0].shape[0] / img_height)\n",
    "        xmax = int(box[-2] * orig_images[0].shape[1] / img_width)\n",
    "        ymax = int(box[-1] * orig_images[0].shape[0] / img_height)\n",
    "       \n",
    "\n",
    "      \n",
    "        orig_images[0]= cv2.rectangle(orig_images[0],(xmin, ymin), (xmax, ymax),(0,255,255),2)\n",
    "      \n",
    "\n",
    "\n",
    "    cv2.imshow(\"image1\",orig_images[0])\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = ssd_300(\"training\",\n",
    "                image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                limit_boxes=limit_boxes,\n",
    "                variances=variances,\n",
    "                coords=coords,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=subtract_mean,\n",
    "                divide_by_stddev=127.5,\n",
    "                swap_channels=swap_channels)\n",
    "    model.load_weights(\"ssd300_epoch-222.h5\")\n",
    "    dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "    VOC_2007_images_dir = 'data/training/'\n",
    "    VOC_2007_images_dir1 = 'data/val/'\n",
    "  \n",
    "\n",
    "  # The directories that contain the annotations.\n",
    "    VOC_2007_annotations_dir = 'annotrain/'\n",
    "    VOC_2007_annotations_dir1 = 'annoval/'\n",
    "  \n",
    "\n",
    "  # The paths to the image sets.\n",
    " # VOC_2007_train_image_set_filename = args.voc_dir_path + '/VOC2007/ImageSets/Main/trainval.txt'\n",
    "\n",
    "\n",
    " # VOC_2007_val_image_set_filename = args.voc_dir_path + '/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "\n",
    "  # The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "\n",
    "    classes = ['drinking','eating']\n",
    "    VOC_2007_train_image_set_filename= 'data1.txt'\n",
    "    VOC_2007_train_val_set_filename= 'data2.txt'\n",
    "\n",
    "\n",
    "    \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    # VOC_2012_val_image_set_filename = '/media/shareit/manish/blitznet-master/Datasets/VOCdevkit/VOC2012/ImageSets/Main/test.txt'\n",
    "\n",
    "\n",
    "    # The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "    \n",
    "    classes = ['drinking','eating']\n",
    "    conf_threshold = 0.25\n",
    "\n",
    "\n",
    "    filenames, labels, image_ids = dataset.parse_xml(images_dirs=[VOC_2007_images_dir1],\n",
    "                        image_set_filenames=[VOC_2007_train_val_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir1],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=True\n",
    "                        )\n",
    "\n",
    "    size = len(filenames)\n",
    "    detected_labels = []\n",
    "    \n",
    "\n",
    "\n",
    "    all_detections = [[None for i in range(len(classes))] for j in range(size)]\n",
    "    all_annotations = [[None for i in range(len(classes))] for j in range(size)]\n",
    "\n",
    "    for i in range(size):\n",
    "\n",
    "        image_path = filenames[i]\n",
    "        ima = cv2.imread(image_path)\n",
    "        orig_images = []\n",
    "\n",
    "        orig_images.append(ima)\n",
    "\n",
    "        image1 = cv2.resize(ima,(img_height,img_width))\n",
    "        image1 = image1[np.newaxis,:,:,:]\n",
    "\n",
    "        input_images = np.array(image1)\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(input_images)\n",
    "        print(\"Time Taken by ssd\", time.time() - start_time) \n",
    "\n",
    "        y_pred_decoded = decode_y(y_pred,\n",
    "                                  confidence_thresh=0.15,\n",
    "                                  iou_threshold=0.75,\n",
    "                                  top_k=100,\n",
    "                                  input_coords='centroids',\n",
    "                                  normalize_coords=True,\n",
    "                                  img_height=img_height,\n",
    "                                  img_width=img_width)\n",
    "\n",
    "        pred_boxes = []\n",
    "        pred_labels = []\n",
    "\n",
    "        for box in y_pred_decoded[0]:\n",
    "\n",
    "            xmin = int(box[-4] * orig_images[0].shape[1] / img_width)\n",
    "            ymin = int(box[-3] * orig_images[0].shape[0] / img_height)\n",
    "            xmax = int(box[-2] * orig_images[0].shape[1] / img_width)\n",
    "            ymax = int(box[-1] * orig_images[0].shape[0] / img_height)\n",
    "            class_id = int(box[0])\n",
    "            score = box[1]\n",
    "\n",
    "            pred_boxes.append([xmin, ymin, xmax, ymax, score])\n",
    "\n",
    "            pred_labels.append(class_id)\n",
    "\n",
    "        pred_boxes = np.array(pred_boxes)\n",
    "        pred_labels = np.array(pred_labels)\n",
    "\n",
    "        l = range(1, len(classes))\n",
    "        for label in l:\n",
    "            if(len(pred_labels)):\n",
    "                all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "\n",
    "        true_label = np.array(labels[i])\n",
    "        \n",
    "        for label in l:\n",
    "            if len(true_label) > 0:\n",
    "                all_annotations[i][label] = true_label[true_label[:, 0] == label, 1:5].copy()\n",
    "            else:\n",
    "                all_annotations[i][label] = np.array([[]])\n",
    "\n",
    "    average_precisions = {}\n",
    "\n",
    "\n",
    "    for label in l:\n",
    "        false_positives = np.zeros((0,))\n",
    "        true_positives = np.zeros((0,))\n",
    "        scores = np.zeros((0,))\n",
    "        num_annotations = 0.0\n",
    "\n",
    "        for i in range(size):\n",
    "            annotations = all_annotations[i][label]\n",
    "            annotations = annotations.astype(np.float32)\n",
    "\n",
    "\n",
    "            num_annotations += annotations.shape[0]\n",
    "            detected_annotations = []\n",
    "            detections = all_detections[i][label]\n",
    "            if(detections is not None):\n",
    "                detections = detections.astype(np.float32)\n",
    "\n",
    "                for d in detections:\n",
    "                    scores = np.append(scores, d[4])\n",
    "\n",
    "                    try:\n",
    "                        annotations[0][0]\n",
    "                    except IndexError:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives = np.append(true_positives, 0)\n",
    "                        continue\n",
    "\n",
    "                    overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                    assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                    max_overlap = overlaps[0, assigned_annotation]\n",
    "                    \n",
    "                    if max_overlap >= conf_threshold and assigned_annotation not in detected_annotations:\n",
    "                    \n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives = np.append(true_positives, 1)\n",
    "                        detected_annotations.append(assigned_annotation)\n",
    "                    else:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives = np.append(true_positives, 0)\n",
    "\n",
    "        \n",
    "        if num_annotations == 0:\n",
    "            average_precisions[label] = 0\n",
    "            continue\n",
    "        indices = np.argsort(-scores)\n",
    "        false_positives = false_positives[indices]\n",
    "        true_positives = true_positives[indices]\n",
    "\n",
    "        false_positives = np.cumsum(false_positives)\n",
    "        true_positives = np.cumsum(true_positives)\n",
    "\n",
    "        recall = true_positives / num_annotations\n",
    "        \n",
    "        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "        average_precision = compute_ap(recall, precision)\n",
    "        average_precisions[label] = average_precision\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for k in average_precisions.keys():\n",
    "        count  = count + float(average_precisions[k])\n",
    "\n",
    "\n",
    "\n",
    "    map1 = count/len(l)\n",
    "    print(average_precisions) \n",
    "    print('MAP is :' , map1)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "   \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
